{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import countDistinct, avg\n",
    "from pyspark.sql.functions import dayofmonth,dayofyear,year,month,hour,weekofyear,date_format\n",
    "from pyspark.sql.functions import col as func_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Edureka_121039'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "for part in cwd.split('/'):\n",
    "    if part.lower().startswith('edureka'):\n",
    "        user_id = part.title()\n",
    "user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Edureka_121039 : Spark SQL'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_name = '{0} : Spark SQL'.format(user_id)\n",
    "app_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark.sparkContext.stop()\n",
    "# spark.stop()\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate() # singleton instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hdfs_filepath(file_name):\n",
    "    my_hdfs = '/user/{0}'.format(user_id.lower())\n",
    "    return os.path.join(my_hdfs, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "Ensure you save all data files under dir <font color='red'>**/user/edureka_(your-user-id)/**</font> on HDFS using Hue browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMP_JSON = get_hdfs_filepath('emp.json')\n",
    "EMP_CSV = get_hdfs_filepath('emp.csv') # comma delimited\n",
    "DEPT_CSV = get_hdfs_filepath('dept.csv')\n",
    "\n",
    "PEOPLE_JSON = get_hdfs_filepath('people.json')\n",
    "GOOG_CSV = get_hdfs_filepath('goog_stock.csv')\n",
    "NESTED_JSON = get_hdfs_filepath('nested.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp_json_df = spark.read.json(EMP_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+\n",
      "|age| id|  name|salary|\n",
      "+---+---+------+------+\n",
      "| 30|  1|  Bill| 10000|\n",
      "| 40|  2| Steve| 12000|\n",
      "| 50|  3|Donald| 14000|\n",
      "| 60|  4|  Modi| 18000|\n",
      "| 70|  5|Sunder| 22000|\n",
      "| 80|  6|  Jeff| 26000|\n",
      "| 90|  7|Sergey| 30000|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'id', 'name', 'salary']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_json_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age: string, id: string, name: string, salary: string]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_json_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------+-----------------+\n",
      "|summary|               age|               id|  name|           salary|\n",
      "+-------+------------------+-----------------+------+-----------------+\n",
      "|  count|                 7|                7|     7|                7|\n",
      "|   mean|              60.0|              4.0|  null|18857.14285714286|\n",
      "| stddev|21.602468994692867|2.160246899469287|  null|7470.577207252718|\n",
      "|    min|                30|                1|  Bill|            10000|\n",
      "|    max|                90|                7|Sunder|            30000|\n",
      "+-------+------------------+-----------------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_json_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<salary>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_json_df['salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emp_json_df['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading nested JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nested = spark.read.json(NESTED_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phones: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- reporting: struct (nullable = true)\n",
      " |    |-- manager: string (nullable = true)\n",
      " |    |-- rm: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|  col| id|\n",
      "+-----+---+\n",
      "|12345|  1|\n",
      "|56789|  1|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phones = nested.select([explode('phones'), 'id'])\n",
    "phones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "report = nested.select([nested['reporting'].alias('tmp'),nested['id'],nested['age']]).select(['tmp.*', 'id', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---+---+\n",
      "|manager|   rm| id|age|\n",
      "+-------+-----+---+---+\n",
      "|  Steve|Brian|  1| 30|\n",
      "+-------+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Column DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[salary: bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_json_df.select('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "| 10000|\n",
      "| 12000|\n",
      "| 14000|\n",
      "| 18000|\n",
      "| 22000|\n",
      "| 26000|\n",
      "| 30000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_json_df.select('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emp_json_df.select('salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=30, id=1, name=u'Bill', salary=10000),\n",
       " Row(age=40, id=2, name=u'Steve', salary=12000),\n",
       " Row(age=50, id=3, name=u'Donald', salary=14000)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_json_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+\n",
      "|age| id| name|salary|\n",
      "+---+---+-----+------+\n",
      "| 30|  1| Bill| 10000|\n",
      "| 40|  2|Steve| 12000|\n",
      "+---+---+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_json_df.show(2,truncate= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=30, id=1, name=u'Bill', salary=10000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emp_json_df.head(2)[0])\n",
    "emp_json_df.head(2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_json_df.select(['age','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Bill|\n",
      "| 40| Steve|\n",
      "| 50|Donald|\n",
      "| 60|  Modi|\n",
      "| 70|Sunder|\n",
      "| 80|  Jeff|\n",
      "| 90|Sergey|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_json_df.select(['age','name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|emp_age|  name|rev_sal|\n",
      "+-------+------+-------+\n",
      "|     30|  Bill|10500.0|\n",
      "|     40| Steve|12600.0|\n",
      "|     50|Donald|14700.0|\n",
      "|     60|  Modi|18900.0|\n",
      "|     70|Sunder|23100.0|\n",
      "|     80|  Jeff|27300.0|\n",
      "|     90|Sergey|31500.0|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias column names\n",
    "emp_json_df.select(emp_json_df.age.alias('emp_age'), emp_json_df.name, (emp_json_df.salary * 1.05).alias('rev_sal')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emp_json_df.['newsal'] = emp_json_df['salary'] * 1.05 # TypeError: 'DataFrame' object does not support item assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+-------+\n",
      "|age| id|  name|salary| newsal|\n",
      "+---+---+------+------+-------+\n",
      "| 30|  1|  Bill| 10000|10500.0|\n",
      "| 40|  2| Steve| 12000|12600.0|\n",
      "| 50|  3|Donald| 14000|14700.0|\n",
      "| 60|  4|  Modi| 18000|18900.0|\n",
      "| 70|  5|Sunder| 22000|23100.0|\n",
      "| 80|  6|  Jeff| 26000|27300.0|\n",
      "| 90|  7|Sergey| 30000|31500.0|\n",
      "+---+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instead, use withColumn method to add a new column (with/without a transformation)\n",
    "emp_json_df.withColumn('newsal', emp_json_df['salary'] * 1.05).show()\n",
    "# OR\n",
    "# emp_json_df.withColumn('newsal', emp_json_df['salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, you may execute withColumn multiple times as it just overwrites the same column name everytime without any error.\n",
    "Also, the changes made here are not permanent to DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'id', 'name', 'salary', 'newsal']\n"
     ]
    }
   ],
   "source": [
    "print(emp_json_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Changes to DF using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+-------+\n",
      "|age| id|  name|salary| newsal|\n",
      "+---+---+------+------+-------+\n",
      "| 30|  1|  Bill| 10000|10500.0|\n",
      "| 40|  2| Steve| 12000|12600.0|\n",
      "| 50|  3|Donald| 14000|14700.0|\n",
      "| 60|  4|  Modi| 18000|18900.0|\n",
      "| 70|  5|Sunder| 22000|23100.0|\n",
      "| 80|  6|  Jeff| 26000|27300.0|\n",
      "| 90|  7|Sergey| 30000|31500.0|\n",
      "+---+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make changes permanent\n",
    "emp_json_df = emp_json_df.withColumn('newsal', emp_json_df['salary'] * 1.05)\n",
    "emp_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+-----------+\n",
      "|age| id|  name|salary|revised_sal|\n",
      "+---+---+------+------+-----------+\n",
      "| 30|  1|  Bill| 10000|    10500.0|\n",
      "| 40|  2| Steve| 12000|    12600.0|\n",
      "| 50|  3|Donald| 14000|    14700.0|\n",
      "| 60|  4|  Modi| 18000|    18900.0|\n",
      "| 70|  5|Sunder| 22000|    23100.0|\n",
      "| 80|  6|  Jeff| 26000|    27300.0|\n",
      "| 90|  7|Sergey| 30000|    31500.0|\n",
      "+---+---+------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Rename\n",
    "emp_json_df.withColumnRenamed('newsal', 'revised_sal').show() # does not result in error, if col is missing in DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL\n",
    "To use SQL queries over dataframe, you will need to register it as a temporary view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+-------+\n",
      "|age| id|  name|salary| newsal|\n",
      "+---+---+------+------+-------+\n",
      "| 30|  1|  Bill| 10000|10500.0|\n",
      "| 40|  2| Steve| 12000|12600.0|\n",
      "| 50|  3|Donald| 14000|14700.0|\n",
      "| 60|  4|  Modi| 18000|18900.0|\n",
      "| 70|  5|Sunder| 22000|23100.0|\n",
      "| 80|  6|  Jeff| 26000|27300.0|\n",
      "| 90|  7|Sergey| 30000|31500.0|\n",
      "+---+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_json_df.createOrReplaceTempView('Emp')\n",
    "sql_df = spark.sql('Select * From Emp')\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|salary|rev_sal|\n",
      "+------+-------+\n",
      "| 18000|19800.0|\n",
      "| 22000|24200.0|\n",
      "| 26000|28600.0|\n",
      "| 30000|33000.0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# more complex ops\n",
    "result = spark.sql('Select salary, salary*1.1 as rev_sal From emp Where salary >= 18000')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5>******</font> Leverage your SQL knowledge over Spark SQL and do complex operations more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dataframe Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let Spark know about the header and infer the Schema types!\n",
    "# This is only available as option on CSVs and not on JSON files.\n",
    "goog_df = spark.read.csv(GOOG_CSV,inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goog_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039),\n",
       " Row(Date=datetime.datetime(2010, 1, 5, 0, 0), Open=214.599998, High=215.589994, Low=213.249994, Close=214.379993, Volume=150476200, Adj Close=27.774976000000002),\n",
       " Row(Date=datetime.datetime(2010, 1, 6, 0, 0), Open=214.379993, High=215.23, Low=210.750004, Close=210.969995, Volume=138040000, Adj Close=27.333178000000004)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goog_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|                Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+--------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:...|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:...|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:...|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:...|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:...|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11 00:00:...|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12 00:00:...|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13 00:00:...|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14 00:00:...|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15 00:00:...|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19 00:00:...|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20 00:00:...|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21 00:00:...|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-25 00:00:...|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26 00:00:...|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27 00:00:...|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-02-12 00:00:...|        198.109995|        201.639996|        195.500002|200.37999299999998|163867200|25.961142000000002|\n",
      "|2010-02-16 00:00:...|        201.940002|        203.690002|        201.520006|        203.399996|135934400|         26.352412|\n",
      "|2010-02-17 00:00:...|        204.190001|        204.310003|        200.860004|        202.550003|109099200|         26.242287|\n",
      "|2010-02-18 00:00:...|        201.629995|        203.889994|        200.920006|        202.929998|105706300|         26.291519|\n",
      "+--------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "goog_df.filter(\"Close>200\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total records\n",
    "goog_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|              Open|   Volume|\n",
      "+------------------+---------+\n",
      "|        213.429998|123432400|\n",
      "|        214.599998|150476200|\n",
      "|        214.379993|138040000|\n",
      "|            211.75|119282800|\n",
      "|        210.299994|111902700|\n",
      "|212.79999700000002|115557400|\n",
      "|209.18999499999998|148614900|\n",
      "|        207.870005|151473000|\n",
      "|210.11000299999998|108223500|\n",
      "|210.92999500000002|148516900|\n",
      "|        208.330002|182501900|\n",
      "|        214.910006|153038200|\n",
      "|        212.079994|152038600|\n",
      "|202.51000200000001|266424900|\n",
      "|205.95000100000001|466777500|\n",
      "|        206.849995|430642100|\n",
      "|        198.109995|163867200|\n",
      "|        201.940002|135934400|\n",
      "|        204.190001|109099200|\n",
      "|        201.629995|105706300|\n",
      "+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using SQL with select\n",
    "goog_df.filter(\"Close>200\").select(['Open','Volume']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex filtering using Python operators for comparison\n",
    "Syntax looks very similar to SQL operators, except we need to ensure that we call the entire column within the dataframe, using the format: df[\"column name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-01dd6008da58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Common mistake with syntax : will produce an error, read the error to address issue here!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgoog_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoog_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Close\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgoog_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Open'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/column.pyc\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         raise ValueError(\"Cannot convert column into bool: please use '&' for 'and', '|' for 'or', \"\n\u001b[0m\u001b[1;32m    427\u001b[0m                          \"'~' for 'not' when building DataFrame boolean expressions.\")\n\u001b[1;32m    428\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "# Common mistake with syntax : will produce an error, read the error to address issue here!\n",
    "goog_df.filter(goog_df[\"Close\"] < 200 and goog_df['Open'] > 200).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|                Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+--------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22 00:00:...|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28 00:00:...|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:...|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
      "+--------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resolution: add in the parenthesis separating the statements.\n",
    "goog_df.filter( (goog_df[\"Close\"] < 200) & (goog_df['Open'] > 200)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Operators\n",
    "* |   -> or\n",
    "* &   -> and\n",
    "* ~   -> not (equivalent to ! in Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# goog_df.filter( (goog_df[\"Close\"] < 200) | (goog_df['Open'] > 200) ).show()\n",
    "# goog_df.filter( (goog_df[\"Close\"] < 200) & ~(goog_df['Open'] < 200) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+------+------+---------+---------+\n",
      "|                Date|              Open|      High|   Low| Close|   Volume|Adj Close|\n",
      "+--------------------+------------------+----------+------+------+---------+---------+\n",
      "|2010-01-22 00:00:...|206.78000600000001|207.499996|197.16|197.75|220441900|25.620401|\n",
      "+--------------------+------------------+----------+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specific value search\n",
    "goog_df.filter(goog_df[\"Low\"] == 197.16).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = goog_df.filter(goog_df[\"Low\"] == 197.16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result[0])\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows can be converted to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adj Close': 25.620401,\n",
       " 'Close': 197.75,\n",
       " 'Date': datetime.datetime(2010, 1, 22, 0, 0),\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Open': 206.78000600000001,\n",
       " 'Volume': 220441900}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-22 00:00:00\n",
      "206.780006\n",
      "207.499996\n",
      "197.16\n",
      "197.75\n",
      "220441900\n",
      "25.620401\n"
     ]
    }
   ],
   "source": [
    "# get all values\n",
    "for item in row:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy, Agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read employee data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cemp_df = spark.read.csv(EMP_CSV,inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cemp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+-------+\n",
      "| id|  name|age|salary|dept_id|\n",
      "+---+------+---+------+-------+\n",
      "|  1|  Bill| 30| 10000|     D1|\n",
      "|  2| Steve| 40| 12000|     D2|\n",
      "|  3|Donald| 50| 14000|     D2|\n",
      "|  4|  Modi| 60| 18000|     D1|\n",
      "|  5|Sundar| 70| 22000|     D3|\n",
      "|  6|  Jeff| 80| 22000|     D3|\n",
      "|  7|Sergey| 90| 30000|     D3|\n",
      "+---+------+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cemp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's group together by dept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fc8d87ff090>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cemp_df.groupBy(\"dept_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a GroupedData object, of which various methods can be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+------------------+\n",
      "|dept_id|avg(id)|avg(age)|       avg(salary)|\n",
      "+-------+-------+--------+------------------+\n",
      "|     D1|    2.5|    45.0|           14000.0|\n",
      "|     D3|    6.0|    80.0|24666.666666666668|\n",
      "|     D2|    2.5|    45.0|           13000.0|\n",
      "+-------+-------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mean\n",
    "cemp_df.groupBy(\"dept_id\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_id|       avg(salary)|\n",
      "+-------+------------------+\n",
      "|     D1|           14000.0|\n",
      "|     D3|24666.666666666668|\n",
      "|     D2|           13000.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cemp_df.groupBy(\"dept_id\").mean('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------+\n",
      "|dept_id|sum(salary)|sum(age)|\n",
      "+-------+-----------+--------+\n",
      "|     D1|      28000|      90|\n",
      "|     D3|      74000|     240|\n",
      "|     D2|      26000|      90|\n",
      "+-------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applying sum over multiple columns at once\n",
    "cemp_df.groupBy(\"dept_id\").sum(\"salary\", \"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alias name for aggregated measure columns\n",
    "\n",
    "In below example, 'alias' method has no impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_id|       avg(salary)|\n",
      "+-------+------------------+\n",
      "|     D1|           14000.0|\n",
      "|     D3|24666.666666666668|\n",
      "|     D2|           13000.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cemp_df.groupBy(\"dept_id\").mean('salary').alias('avg sal').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating alias, we use **pyspark.sql.functions.col** imported as func_col above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_id|            avgSal|\n",
      "+-------+------------------+\n",
      "|     D1|           14000.0|\n",
      "|     D3|24666.666666666668|\n",
      "|     D2|           13000.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cemp_df.groupBy(\"dept_id\").mean('salary').select('dept_id',func_col(\"avg(salary)\").alias(\"avgSal\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, **withColumnRenamed** can be used to rename an aggregate measure and generate alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|dept_id|            avgSal|\n",
      "+-------+------------------+\n",
      "|     D1|           14000.0|\n",
      "|     D3|24666.666666666668|\n",
      "|     D2|           13000.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cemp_df.groupBy(\"dept_id\").mean('salary').withColumnRenamed('avg(salary)', 'avgSal').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Multiple aggregations cannot be combined with group-by as independent functions.</font>\n",
    "\n",
    "**Hint** : Try using <font color='red'>**agg**</font> instead to apply different aggregation measures on a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ae2d27107c7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# applying sum and mean together on columns at once - results in an ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcemp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dept_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"age\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 964\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "# applying sum and mean together on columns at once - results in an ERROR\n",
    "cemp_df.groupBy(\"dept_id\").sum(\"salary\", \"age\").mean(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|dept_id|count|\n",
      "+-------+-----+\n",
      "|     D1|    2|\n",
      "|     D3|    3|\n",
      "|     D2|    2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count\n",
    "cemp_df.groupBy(\"dept_id\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+\n",
      "|dept_id|max(id)|max(age)|max(salary)|\n",
      "+-------+-------+--------+-----------+\n",
      "|     D1|      4|      60|      18000|\n",
      "|     D3|      7|      90|      30000|\n",
      "|     D2|      3|      50|      14000|\n",
      "+-------+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max\n",
    "cemp_df.groupBy(\"dept_id\").max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+\n",
      "|dept_id|min(id)|min(age)|min(salary)|\n",
      "+-------+-------+--------+-----------+\n",
      "|     D1|      1|      30|      10000|\n",
      "|     D3|      5|      70|      22000|\n",
      "|     D2|      2|      40|      12000|\n",
      "+-------+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Min\n",
    "cemp_df.groupBy(\"dept_id\").min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+\n",
      "|dept_id|sum(id)|sum(age)|sum(salary)|\n",
      "+-------+-------+--------+-----------+\n",
      "|     D1|      5|      90|      28000|\n",
      "|     D3|     18|     240|      74000|\n",
      "|     D2|      5|      90|      26000|\n",
      "+-------+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sum\n",
    "cemp_df.groupBy(\"dept_id\").sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this link for more info on other methods:\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating without any Group By\n",
    "\n",
    "Not all methods or analysis would always require a group-by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|      30000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max salary across everything\n",
    "cemp_df.agg({'salary':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way of using Group By\n",
    "\n",
    "Could try this using group by as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = cemp_df.groupBy(\"dept_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|dept_id|max(salary)|\n",
      "+-------+-----------+\n",
      "|     D1|      18000|\n",
      "|     D3|      30000|\n",
      "|     D2|      14000|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Max salary by dept\n",
    "grouped.agg({\"salary\":'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group By col1, col2, col3\n",
    "grouped = cemp_df.groupBy([\"dept_id\",'salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply aggregation functions on grouped data <font color='red'>using</font> \"agg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single aggregation using **agg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|dept_id|salary|max(age)|\n",
      "+-------+------+--------+\n",
      "|     D1| 18000|      60|\n",
      "|     D2| 14000|      50|\n",
      "|     D3| 30000|      90|\n",
      "|     D3| 22000|      80|\n",
      "|     D2| 12000|      40|\n",
      "|     D1| 10000|      30|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped.agg({\"age\":'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple aggregations using **agg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|dept_id|sum(id)|max(age)|\n",
      "+-------+-------+--------+\n",
      "|     D1|      5|      60|\n",
      "|     D3|     18|      90|\n",
      "|     D2|      5|      50|\n",
      "+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped.agg({'age':'max', 'id': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple aggregations using **agg** along with **alias** on aggregate measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------+-----------+\n",
      "|dept_id|salary|maxAge|minAge|distinctAge|\n",
      "+-------+------+------+------+-----------+\n",
      "|     D1| 18000|    60|    60|          1|\n",
      "|     D2| 14000|    50|    50|          1|\n",
      "|     D3| 30000|    90|    90|          1|\n",
      "|     D3| 22000|    80|    70|          2|\n",
      "|     D2| 12000|    40|    40|          1|\n",
      "|     D1| 10000|    30|    30|          1|\n",
      "+-------+------+------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as smax, min as smin, count, countDistinct\n",
    "\n",
    "grouped.agg(smax(\"age\").alias(\"maxAge\"), \n",
    "            smin(\"age\").alias(\"minAge\"), \n",
    "            countDistinct('age').alias('distinctAge')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply aggregation functions directly on grouped-data <font color='red'>without</font> using \"agg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+-------+\n",
      "|dept_id|salary|max(age)|max(id)|\n",
      "+-------+------+--------+-------+\n",
      "|     D1| 18000|      60|      4|\n",
      "|     D2| 14000|      50|      3|\n",
      "|     D3| 30000|      90|      7|\n",
      "|     D3| 22000|      80|      6|\n",
      "|     D2| 12000|      40|      2|\n",
      "|     D1| 10000|      30|      1|\n",
      "+-------+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped.max('age', 'id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming aggregate measure columns using **withColumnRenamed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+------+\n",
      "|dept_id|salary|max_age|max_id|\n",
      "+-------+------+-------+------+\n",
      "|     D1| 18000|     60|     4|\n",
      "|     D2| 14000|     50|     3|\n",
      "|     D3| 30000|     90|     7|\n",
      "|     D3| 22000|     80|     6|\n",
      "|     D2| 12000|     40|     2|\n",
      "|     D1| 10000|     30|     1|\n",
      "+-------+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped.max('age', 'id').withColumnRenamed('max(age)', 'max_age').withColumnRenamed('max(id)', 'max_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Note**</font> : aggregation functions do not work on strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'\"name\" is not a numeric column. Aggregation function can only be applied on a numeric column.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b107ff8aac80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'\"name\" is not a numeric column. Aggregation function can only be applied on a numeric column.;'"
     ]
    }
   ],
   "source": [
    "grouped.max('age', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Functions\n",
    "Variety of functions that we can import from pyspark.sql.functions. For details refer:\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "|               6|\n",
      "|               7|\n",
      "|               8|\n",
      "|              11|\n",
      "|              12|\n",
      "|              13|\n",
      "|              14|\n",
      "|              15|\n",
      "|              19|\n",
      "|              20|\n",
      "|              21|\n",
      "|              22|\n",
      "|              25|\n",
      "|              26|\n",
      "|              27|\n",
      "|              28|\n",
      "|              29|\n",
      "|               1|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goog_df.select(dayofmonth(goog_df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dayofyear(Date)|\n",
      "+---------------+\n",
      "|              4|\n",
      "|              5|\n",
      "|              6|\n",
      "|              7|\n",
      "|              8|\n",
      "|             11|\n",
      "|             12|\n",
      "|             13|\n",
      "|             14|\n",
      "|             15|\n",
      "|             19|\n",
      "|             20|\n",
      "|             21|\n",
      "|             22|\n",
      "|             25|\n",
      "|             26|\n",
      "|             27|\n",
      "|             28|\n",
      "|             29|\n",
      "|             32|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goog_df.select(dayofyear(goog_df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(Date)|\n",
      "+----------+\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "|      2010|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goog_df.select(year(goog_df['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|month(Date)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          2|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goog_df.select(month('Date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|hour(Date)|\n",
      "+----------+\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goog_df.select(hour('Date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT salary)|\n",
      "+----------------------+\n",
      "|                     6|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cemp_df.select(countDistinct(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Distinct Salaries|\n",
      "+-----------------+\n",
      "|                6|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cemp_df.select(countDistinct(\"salary\").alias(\"Distinct Salaries\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       avg(salary)|\n",
      "+------------------+\n",
      "|18285.714285714286|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cemp_df.select(avg('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|stddev_samp(salary)|\n",
      "+-------------------+\n",
      "|  6969.320524371697|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev\n",
    "cemp_df.select(stddev(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+-------+\n",
      "| id|  name|age|salary|dept_id|\n",
      "+---+------+---+------+-------+\n",
      "|  1|  Bill| 30| 10000|     D1|\n",
      "|  2| Steve| 40| 12000|     D2|\n",
      "|  3|Donald| 50| 14000|     D2|\n",
      "|  4|  Modi| 60| 18000|     D1|\n",
      "|  5|Sundar| 70| 22000|     D3|\n",
      "|  6|  Jeff| 80| 22000|     D3|\n",
      "|  7|Sergey| 90| 30000|     D3|\n",
      "+---+------+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OrderBy\n",
    "# Ascending\n",
    "cemp_df.orderBy(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+-------+\n",
      "| id|  name|age|salary|dept_id|\n",
      "+---+------+---+------+-------+\n",
      "|  7|Sergey| 90| 30000|     D3|\n",
      "|  5|Sundar| 70| 22000|     D3|\n",
      "|  6|  Jeff| 80| 22000|     D3|\n",
      "|  4|  Modi| 60| 18000|     D1|\n",
      "|  3|Donald| 50| 14000|     D2|\n",
      "|  2| Steve| 40| 12000|     D2|\n",
      "|  1|  Bill| 30| 10000|     D1|\n",
      "+---+------+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descending sort\n",
    "cemp_df.orderBy(cemp_df[\"salary\"].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+-------+\n",
      "| id|  name|age|salary|dept_id|\n",
      "+---+------+---+------+-------+\n",
      "|  7|Sergey| 90| 30000|     D3|\n",
      "|  6|  Jeff| 80| 22000|     D3|\n",
      "|  5|Sundar| 70| 22000|     D3|\n",
      "|  4|  Modi| 60| 18000|     D1|\n",
      "|  3|Donald| 50| 14000|     D2|\n",
      "|  2| Steve| 40| 12000|     D2|\n",
      "|  1|  Bill| 30| 10000|     D1|\n",
      "+---+------+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descending and ascending sort on multiple columns\n",
    "cemp_df.orderBy(cemp_df.salary.desc(), cemp_df.name.asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving DF as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_json_df.write.mode('overwrite').parquet(get_hdfs_filepath('emp.parquet'))\n",
    "# emp_json_df.write.parquet(get_hdfs_filepath('emp.parquet')) # expect exception if the parquet file already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=30, id=1, name=u'Bill', salary=10000, newsal=10500.0),\n",
       " Row(age=40, id=2, name=u'Steve', salary=12000, newsal=12600.0),\n",
       " Row(age=50, id=3, name=u'Donald', salary=14000, newsal=14700.0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pemp_df = spark.read.parquet('emp.parquet')\n",
    "pemp_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup SQLContext to register UDF\n",
    "from pyspark.sql import SQLContext\n",
    "sc = spark.sparkContext\n",
    "sql_ctx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# register UDF\n",
    "sql_ctx.registerFunction('ucase', lambda val: val.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+------+-------+\n",
      "|age| id|  name|salary| newsal|\n",
      "+---+---+------+------+-------+\n",
      "| 60|  4|  Modi| 18000|18900.0|\n",
      "| 70|  5|Sunder| 22000|23100.0|\n",
      "| 80|  6|  Jeff| 26000|27300.0|\n",
      "| 90|  7|Sergey| 30000|31500.0|\n",
      "+---+---+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regular query on temp view we created earlier.\n",
    "spark.sql('Select * From emp Where salary >= 18000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+\n",
      "| uname|salary|age|\n",
      "+------+------+---+\n",
      "|  MODI| 18000| 60|\n",
      "|SUNDER| 22000| 70|\n",
      "|  JEFF| 26000| 80|\n",
      "|SERGEY| 30000| 90|\n",
      "+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query using registered UDF to transform output\n",
    "spark.sql('Select ucase(name) uname, salary, age From emp Where salary >= 18000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stops spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
