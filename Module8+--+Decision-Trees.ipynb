{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkConf,SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Edureka_121039'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "for part in cwd.split('/'):\n",
    "    if part.lower().startswith('edureka'):\n",
    "        user_id = part.title()\n",
    "user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Edureka_121039 : Decision Trees'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_name = '{0} : Decision Trees'.format(user_id)\n",
    "app_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "sparkContext = spark.sparkContext\n",
    "sqlContext = SQLContext(sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hdfs_filepath(file_name):\n",
    "    my_hdfs = '/user/{0}/datasets'.format(user_id.lower())\n",
    "    return os.path.join(my_hdfs, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Details can be referred [here](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset \"Bike Sharing dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BIKE_SHARING_CSV = get_hdfs_filepath('bikeSharing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(BIKE_SHARING_CSV,inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[instant: int, dteday: timestamp, season: int, yr: int, mnth: int, hr: int, holiday: int, weekday: int, workingday: int, weathersit: int, temp: double, atemp: double, hum: double, windspeed: double, casual: int, registered: int, cnt: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|instant|              dteday|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|casual|registered|cnt|\n",
      "+-------+--------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "|      1|2011-01-01 00:00:...|     1|  0|   1|  0|      0|      6|         0|         1|0.24|0.2879|0.81|      0.0|     3|        13| 16|\n",
      "|      2|2011-01-01 00:00:...|     1|  0|   1|  1|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     8|        32| 40|\n",
      "|      3|2011-01-01 00:00:...|     1|  0|   1|  2|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     5|        27| 32|\n",
      "|      4|2011-01-01 00:00:...|     1|  0|   1|  3|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     3|        10| 13|\n",
      "|      5|2011-01-01 00:00:...|     1|  0|   1|  4|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     0|         1|  1|\n",
      "|      6|2011-01-01 00:00:...|     1|  0|   1|  5|      0|      6|         0|         2|0.24|0.2576|0.75|   0.0896|     0|         1|  1|\n",
      "|      7|2011-01-01 00:00:...|     1|  0|   1|  6|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|     2|         0|  2|\n",
      "|      8|2011-01-01 00:00:...|     1|  0|   1|  7|      0|      6|         0|         1| 0.2|0.2576|0.86|      0.0|     1|         2|  3|\n",
      "|      9|2011-01-01 00:00:...|     1|  0|   1|  8|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|     1|         7|  8|\n",
      "|     10|2011-01-01 00:00:...|     1|  0|   1|  9|      0|      6|         0|         1|0.32|0.3485|0.76|      0.0|     8|         6| 14|\n",
      "|     11|2011-01-01 00:00:...|     1|  0|   1| 10|      0|      6|         0|         1|0.38|0.3939|0.76|   0.2537|    12|        24| 36|\n",
      "|     12|2011-01-01 00:00:...|     1|  0|   1| 11|      0|      6|         0|         1|0.36|0.3333|0.81|   0.2836|    26|        30| 56|\n",
      "|     13|2011-01-01 00:00:...|     1|  0|   1| 12|      0|      6|         0|         1|0.42|0.4242|0.77|   0.2836|    29|        55| 84|\n",
      "|     14|2011-01-01 00:00:...|     1|  0|   1| 13|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2985|    47|        47| 94|\n",
      "|     15|2011-01-01 00:00:...|     1|  0|   1| 14|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2836|    35|        71|106|\n",
      "|     16|2011-01-01 00:00:...|     1|  0|   1| 15|      0|      6|         0|         2|0.44|0.4394|0.77|   0.2985|    40|        70|110|\n",
      "|     17|2011-01-01 00:00:...|     1|  0|   1| 16|      0|      6|         0|         2|0.42|0.4242|0.82|   0.2985|    41|        52| 93|\n",
      "|     18|2011-01-01 00:00:...|     1|  0|   1| 17|      0|      6|         0|         2|0.44|0.4394|0.82|   0.2836|    15|        52| 67|\n",
      "|     19|2011-01-01 00:00:...|     1|  0|   1| 18|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537|     9|        26| 35|\n",
      "|     20|2011-01-01 00:00:...|     1|  0|   1| 19|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537|     6|        31| 37|\n",
      "+-------+--------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset has 1000 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our dataset has %d rows.\" % df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[season: int, yr: int, mnth: int, hr: int, holiday: int, weekday: int, workingday: int, weathersit: int, temp: double, atemp: double, hum: double, windspeed: double, cnt: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.drop(\"instant\").drop(\"dteday\").drop(\"casual\").drop(\"registered\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+\n",
      "|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|cnt|\n",
      "+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+\n",
      "|     1|  0|   1|  0|      0|      6|         0|         1|0.24|0.2879|0.81|      0.0| 16|\n",
      "|     1|  0|   1|  1|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0| 40|\n",
      "|     1|  0|   1|  2|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0| 32|\n",
      "|     1|  0|   1|  3|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0| 13|\n",
      "|     1|  0|   1|  4|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|  1|\n",
      "|     1|  0|   1|  5|      0|      6|         0|         2|0.24|0.2576|0.75|   0.0896|  1|\n",
      "|     1|  0|   1|  6|      0|      6|         0|         1|0.22|0.2727| 0.8|      0.0|  2|\n",
      "|     1|  0|   1|  7|      0|      6|         0|         1| 0.2|0.2576|0.86|      0.0|  3|\n",
      "|     1|  0|   1|  8|      0|      6|         0|         1|0.24|0.2879|0.75|      0.0|  8|\n",
      "|     1|  0|   1|  9|      0|      6|         0|         1|0.32|0.3485|0.76|      0.0| 14|\n",
      "|     1|  0|   1| 10|      0|      6|         0|         1|0.38|0.3939|0.76|   0.2537| 36|\n",
      "|     1|  0|   1| 11|      0|      6|         0|         1|0.36|0.3333|0.81|   0.2836| 56|\n",
      "|     1|  0|   1| 12|      0|      6|         0|         1|0.42|0.4242|0.77|   0.2836| 84|\n",
      "|     1|  0|   1| 13|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2985| 94|\n",
      "|     1|  0|   1| 14|      0|      6|         0|         2|0.46|0.4545|0.72|   0.2836|106|\n",
      "|     1|  0|   1| 15|      0|      6|         0|         2|0.44|0.4394|0.77|   0.2985|110|\n",
      "|     1|  0|   1| 16|      0|      6|         0|         2|0.42|0.4242|0.82|   0.2985| 93|\n",
      "|     1|  0|   1| 17|      0|      6|         0|         2|0.44|0.4394|0.82|   0.2836| 67|\n",
      "|     1|  0|   1| 18|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537| 35|\n",
      "|     1|  0|   1| 19|      0|      6|         0|         3|0.42|0.4242|0.88|   0.2537| 37|\n",
      "+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: integer (nullable = true)\n",
      " |-- yr: integer (nullable = true)\n",
      " |-- mnth: integer (nullable = true)\n",
      " |-- hr: integer (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- workingday: integer (nullable = true)\n",
      " |-- weathersit: integer (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col  # for indicating a column using a string in the line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: double (nullable = true)\n",
      " |-- yr: double (nullable = true)\n",
      " |-- mnth: double (nullable = true)\n",
      " |-- hr: double (nullable = true)\n",
      " |-- holiday: double (nullable = true)\n",
      " |-- weekday: double (nullable = true)\n",
      " |-- workingday: double (nullable = true)\n",
      " |-- weathersit: double (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select([col(c).cast(\"double\").alias(c) for c in df.columns])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 707 training examples and 293 test examples.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have %d training examples and %d test examples.\" % (train.count(), test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hr: double, cnt: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.select(\"hr\", \"cnt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresCols = df.columns\n",
    "featuresCols.remove('cnt')\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n",
    "# This identifies categorical features and indexes them.\n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "# Takes the \"features\" column and learns to predict \"cnt\"\n",
    "gbt = GBTRegressor(labelCol=\"cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to test:\n",
    "#  - maxDepth: max depth of each decision tree in the GBT ensemble\n",
    "#  - maxIter: iterations, i.e., number of trees in each GBT ensemble\n",
    "# In this example notebook, we keep these values small. \n",
    "# In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher) and \n",
    "# more trees in the ensemble (>100).\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth, [2, 5]).addGrid(gbt.maxIter, [10, 100]).build()\n",
    "# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n",
    "# Declare the CrossValidator, which runs model tuning for us.\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cnt: double, prediction: double, season: double, yr: double, mnth: double, hr: double, holiday: double, weekday: double, workingday: double, weathersit: double, temp: double, atemp: double, hum: double, windspeed: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+\n",
      "| cnt|        prediction|season| yr|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed|\n",
      "+----+------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+\n",
      "|22.0| 24.28854670989856|   1.0|0.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.04|0.0758|0.57|   0.1045|\n",
      "|33.0|26.401313942358094|   1.0|0.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1818| 0.8|   0.1045|\n",
      "| 5.0|12.174393428860355|   1.0|0.0| 1.0|0.0|    0.0|    1.0|       1.0|       1.0|0.12|0.1212| 0.5|   0.2836|\n",
      "| 5.0|10.072339913508364|   1.0|0.0| 1.0|0.0|    0.0|    2.0|       1.0|       1.0|0.16|0.1818|0.55|   0.1045|\n",
      "| 7.0| 3.074259985483318|   1.0|0.0| 1.0|0.0|    0.0|    3.0|       1.0|       2.0|0.16| 0.197|0.86|   0.0896|\n",
      "| 3.0| 8.958387747728121|   1.0|0.0| 1.0|0.0|    0.0|    3.0|       1.0|       2.0|0.22|0.2727|0.93|      0.0|\n",
      "|14.0|12.281913466760383|   1.0|0.0| 1.0|0.0|    0.0|    5.0|       1.0|       1.0|0.12|0.1364| 0.5|    0.194|\n",
      "|17.0|12.646781588002236|   1.0|0.0| 1.0|0.0|    0.0|    5.0|       1.0|       2.0| 0.2| 0.197|0.64|    0.194|\n",
      "|13.0|19.622292503749776|   1.0|0.0| 1.0|0.0|    0.0|    6.0|       0.0|       1.0|0.04|0.0303|0.45|   0.2537|\n",
      "|28.0|22.006864088027843|   1.0|0.0| 1.0|0.0|    0.0|    6.0|       0.0|       1.0|0.18|0.2424|0.55|      0.0|\n",
      "|13.0| 23.83073393687632|   1.0|0.0| 1.0|1.0|    0.0|    0.0|       0.0|       1.0|0.04|0.0758|0.57|   0.1045|\n",
      "|17.0| 20.68618320717771|   1.0|0.0| 1.0|1.0|    0.0|    0.0|       0.0|       2.0|0.44|0.4394|0.94|   0.2537|\n",
      "| 1.0| 3.020008377171791|   1.0|0.0| 1.0|1.0|    0.0|    1.0|       1.0|       1.0|0.04|0.0455|0.45|    0.194|\n",
      "| 1.0| 6.906227277555507|   1.0|0.0| 1.0|1.0|    0.0|    1.0|       1.0|       1.0|0.12|0.1212| 0.5|   0.2836|\n",
      "| 7.0|3.5903850598605227|   1.0|0.0| 1.0|1.0|    0.0|    3.0|       1.0|       3.0|0.22|0.2273|0.93|   0.1343|\n",
      "|20.0|19.591952290161906|   1.0|0.0| 1.0|1.0|    0.0|    6.0|       0.0|       2.0|0.16| 0.197|0.59|   0.0896|\n",
      "|16.0|12.752779290609542|   1.0|0.0| 1.0|1.0|    1.0|    1.0|       0.0|       2.0| 0.2| 0.197|0.44|    0.194|\n",
      "| 9.0|13.913891496834925|   1.0|0.0| 1.0|2.0|    0.0|    0.0|       0.0|       2.0|0.42|0.4242| 1.0|   0.2836|\n",
      "| 1.0|5.9822102908764245|   1.0|0.0| 1.0|2.0|    0.0|    3.0|       1.0|       1.0|0.14|0.1515|0.86|   0.1343|\n",
      "| 2.0| 9.878787213034464|   1.0|0.0| 1.0|2.0|    0.0|    4.0|       1.0|       1.0|0.14|0.1212| 0.5|   0.3582|\n",
      "+----+------------------+------+---+----+---+-------+-------+----------+----------+----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(predictions.select(\"cnt\", \"prediction\", *featuresCols))\n",
    "predictions.select(\"cnt\", \"prediction\", *featuresCols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE on our test set: %g\" , rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(predictions.select(\"hr\", \"prediction\"))\n",
    "predictions.select(\"hr\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hr: double, prediction: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions.select(\"hr\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c0301af3e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
