{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "# from pyspark.ml.clustering import KMeans\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "for part in cwd.split('/'):\n",
    "    if part.startswith('edureka'):\n",
    "        user_id = part.title()\n",
    "user_id\n",
    "\n",
    "app_name = '{0} : K-means clustering'.format(user_id)\n",
    "app_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "sparkContext = spark.sparkContext\n",
    "sqlContext = SQLContext(sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hdfs_filepath(file_name):\n",
    "    my_hdfs = '/common4all/{0}/may_batch_data/'.format(user_id.lower())\n",
    "    return os.path.join(my_hdfs, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BOSTON_CSV = get_hdfs_filepath('boston_data.csv')\n",
    "# BOSTON_CSV\n",
    "datadir = '/common4all/edureka_200115/may_batch_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------+------+\n",
      "|dt              |lat    |lon     |base  |\n",
      "+----------------+-------+--------+------+\n",
      "|4/1/2014 0:11:00|40.769 |-73.9549|B02512|\n",
      "|4/1/2014 0:17:00|40.7267|-74.0345|B02512|\n",
      "|4/1/2014 0:21:00|40.7316|-73.9873|B02512|\n",
      "|4/1/2014 0:28:00|40.7588|-73.9776|B02512|\n",
      "|4/1/2014 0:33:00|40.7594|-73.9722|B02512|\n",
      "+----------------+-------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-------+--------+------+------------------+\n",
      "|              dt|    lat|     lon|  base|          features|\n",
      "+----------------+-------+--------+------+------------------+\n",
      "|4/1/2014 0:11:00| 40.769|-73.9549|B02512| [40.769,-73.9549]|\n",
      "|4/1/2014 0:17:00|40.7267|-74.0345|B02512|[40.7267,-74.0345]|\n",
      "|4/1/2014 0:21:00|40.7316|-73.9873|B02512|[40.7316,-73.9873]|\n",
      "|4/1/2014 0:28:00|40.7588|-73.9776|B02512|[40.7588,-73.9776]|\n",
      "|4/1/2014 0:33:00|40.7594|-73.9722|B02512|[40.7594,-73.9722]|\n",
      "+----------------+-------+--------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+-------+--------+------+------------------+----------+\n",
      "|              dt|    lat|     lon|  base|          features|prediction|\n",
      "+----------------+-------+--------+------+------------------+----------+\n",
      "|4/1/2014 0:11:00| 40.769|-73.9549|B02512| [40.769,-73.9549]|         0|\n",
      "|4/1/2014 0:17:00|40.7267|-74.0345|B02512|[40.7267,-74.0345]|         0|\n",
      "|4/1/2014 0:21:00|40.7316|-73.9873|B02512|[40.7316,-73.9873]|         0|\n",
      "|4/1/2014 0:28:00|40.7588|-73.9776|B02512|[40.7588,-73.9776]|         0|\n",
      "|4/1/2014 0:33:00|40.7594|-73.9722|B02512|[40.7594,-73.9722]|         0|\n",
      "|4/1/2014 0:33:00|40.7383|-74.0403|B02512|[40.7383,-74.0403]|         0|\n",
      "|4/1/2014 0:39:00|40.7223|-73.9887|B02512|[40.7223,-73.9887]|         0|\n",
      "|4/1/2014 0:45:00| 40.762| -73.979|B02512|  [40.762,-73.979]|         0|\n",
      "|4/1/2014 0:55:00|40.7524| -73.996|B02512| [40.7524,-73.996]|         0|\n",
      "|4/1/2014 1:01:00|40.7575|-73.9846|B02512|[40.7575,-73.9846]|         0|\n",
      "|4/1/2014 1:19:00|40.7256|-73.9869|B02512|[40.7256,-73.9869]|         0|\n",
      "|4/1/2014 1:48:00|40.7591|-73.9684|B02512|[40.7591,-73.9684]|         0|\n",
      "|4/1/2014 1:49:00|40.7271|-73.9803|B02512|[40.7271,-73.9803]|         0|\n",
      "|4/1/2014 2:11:00|40.6463|-73.7896|B02512|[40.6463,-73.7896]|         3|\n",
      "|4/1/2014 2:25:00|40.7564|-73.9167|B02512|[40.7564,-73.9167]|         0|\n",
      "|4/1/2014 2:31:00|40.7666|-73.9531|B02512|[40.7666,-73.9531]|         0|\n",
      "|4/1/2014 2:43:00| 40.758|-73.9761|B02512| [40.758,-73.9761]|         0|\n",
      "|4/1/2014 3:22:00|40.7238|-73.9821|B02512|[40.7238,-73.9821]|         0|\n",
      "|4/1/2014 3:35:00|40.7531|-74.0039|B02512|[40.7531,-74.0039]|         0|\n",
      "|4/1/2014 3:35:00|40.7389|-74.0393|B02512|[40.7389,-74.0393]|         0|\n",
      "+----------------+-------+--------+------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Final Centers: \n"
     ]
    }
   ],
   "source": [
    "# val schema = StructType(Array(\n",
    "# StructField(\"dt\", StringType, true),\n",
    "# StructField(\"lat\", DoubleType, true),\n",
    "# StructField(\"lon\", DoubleType, true),\n",
    "# StructField(\"base\", StringType, true)))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"dt\", StringType()),\n",
    "    StructField(\"lat\", DoubleType()),\n",
    "    StructField(\"lon\", DoubleType()),\n",
    "    StructField(\"base\", StringType())            \n",
    "])\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(datadir + \"uber-raw-data-apr14.csv\")\n",
    "df.show(5,False)\n",
    "featureCols = [\"lat\", \"lon\"]\n",
    "\n",
    "# VectorAssembler is a transformer that combines a given list of columns into a single vector column.\n",
    "# It is useful for combining raw features and features generated by different feature transformers into a single\n",
    "# feature vector, in order to train ML models like logistic regression and decision trees.\n",
    "# assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "\n",
    "df2 = assembler.transform(df)\n",
    "df2.show(5)\n",
    "kmeans = KMeans(k=5, seed=1)\n",
    "model = kmeans.fit(df2.select('features'))\n",
    "\n",
    "transformed = model.transform(df2)\n",
    "transformed.show() \n",
    "# kmeans = KMeans(k=2, seed=1)  # 2 clusters here\n",
    "# model = kmeans.fit(new_df.select('features'))\n",
    "\n",
    "# val Array(trainingData, testData) = df2.randomSplit(Array(0.7, 0.3), 8743)\n",
    "## kmeans = KMeans(k=2, seed=1)\n",
    "# kmeans = new KMeans().setK(10).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\n",
    "# model = kmeans.fit(df2)\n",
    "## model = kmeans.fit(df2.select('features'))\n",
    "\n",
    "print(\"Final Centers: \")\n",
    "## model.clusterCenters.foreach(println)\n",
    "# clusters = KMeans.train(rdd, 2, maxIterations=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.select('lat', 'lon').rdd.map(lambda x: (x[0], x[1]))\n",
    "clusters = KMeans.train(rdd, 2, maxIterations=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 40.74090043, -73.98586888]), array([ 40.72439491, -73.81897293])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
