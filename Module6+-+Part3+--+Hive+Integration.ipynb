{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from os.path import expanduser, join\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Edureka_121039 : Hive Integration'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "for part in cwd.split('/'):\n",
    "    if part.lower().startswith('edureka'):\n",
    "        user_id = part.title()\n",
    "app_name = '{0} : Hive Integration'.format(user_id)\n",
    "app_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "sparkContext = spark.sparkContext\n",
    "sqlContext = SQLContext(sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hdfs_filepath(file_name):\n",
    "    my_hdfs = '/user/{0}'.format(user_id.lower())\n",
    "    return os.path.join(my_hdfs, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_TXT = get_hdfs_filepath('sampleData.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer warehouse location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.dynamicAllocation.enabled', u'false'),\n",
       " (u'spark.eventLog.enabled', u'true'),\n",
       " (u'spark.port.maxRetries', u'1000'),\n",
       " (u'spark.yarn.jars',\n",
       "  u'local:/opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/lib/spark2/jars/*'),\n",
       " (u'spark.executorEnv.PYTHONPATH',\n",
       "  u'/opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/lib/spark2/python/lib/py4j-0.10.4-src.zip<CPS>/opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/lib/spark2/python/lib/pyspark.zip'),\n",
       " (u'spark.executor.extraLibraryPath',\n",
       "  u'/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/lib/native'),\n",
       " (u'spark.ui.killEnabled', u'true'),\n",
       " (u'spark.eventLog.dir', u'hdfs://nameservice1/user/spark/applicationHistory'),\n",
       " (u'spark.dynamicAllocation.executorIdleTimeout', u'60'),\n",
       " (u'spark.serializer', u'org.apache.spark.serializer.KryoSerializer'),\n",
       " (u'spark.authenticate', u'false'),\n",
       " (u'spark.sql.hive.metastore.jars',\n",
       "  u'${env:HADOOP_COMMON_HOME}/../hive/lib/*:${env:HADOOP_COMMON_HOME}/client/*'),\n",
       " (u'spark.driver.host', u'20.0.41.62'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.driver.port', u'38458'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.ui.filters',\n",
       "  u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " (u'spark.driver.extraLibraryPath',\n",
       "  u'/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/lib/native'),\n",
       " (u'spark.yarn.historyServer.address',\n",
       "  u'http://ip-20-0-21-196.ec2.internal:18089'),\n",
       " (u'spark.shuffle.service.enabled', u'true'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.dynamicAllocation.schedulerBacklogTimeout', u'1'),\n",
       " (u'spark.shuffle.service.port', u'7337'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  u'ip-20-0-21-161.ec2.internal,ip-20-0-21-196.ec2.internal'),\n",
       " (u'spark.sql.hive.metastore.version', u'1.1.0'),\n",
       " (u'spark.hadoop.yarn.application.classpath', u''),\n",
       " (u'spark.driver.memory', u'512m'),\n",
       " (u'spark.yarn.config.gatewayPath', u'/opt/cloudera/parcels'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  u'http://ip-20-0-21-161.ec2.internal:8088/proxy/application_1528714825862_63498,http://ip-20-0-21-196.ec2.internal:8088/proxy/application_1528714825862_63498'),\n",
       " (u'spark.master', u'yarn'),\n",
       " (u'spark.app.name', u'Edureka_121039 : Hive Integration'),\n",
       " (u'spark.sql.warehouse.dir', u'/user/hive/warehouse'),\n",
       " (u'spark.sql.catalogImplementation', u'hive'),\n",
       " (u'spark.hadoop.mapreduce.application.classpath', u''),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.app.id', u'application_1528714825862_63498'),\n",
       " (u'spark.ui.enabled', u'false'),\n",
       " (u'spark.ui.proxyBase', u'/proxy/application_1528714825862_63498'),\n",
       " (u'spark.dynamicAllocation.minExecutors', u'0'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.yarn.config.replacementPath', u'{{HADOOP_COMMON_HOME}}/../../..'),\n",
       " (u'spark.yarn.am.extraLibraryPath',\n",
       "  u'/opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop/lib/native')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()\n",
    "# can be over-written using below\n",
    "# SparkSession.builder.appName().config(\"spark.sql.warehouse.dir\", \"warehouse-location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive Metastore\n",
    "Spark SQL uses a Hive metastore to manage the metadata of persistent relational entities (e.g. databases, tables, columns, partitions) in a relational database (for fast access).\n",
    "\n",
    "A Hive metastore warehouse (aka spark-warehouse) is the directory where Spark SQL persists tables whereas a Hive metastore (aka metastore_db) is a relational database to manage the metadata of the persistent relational entities, e.g. databases, tables, columns, partitions.\n",
    "\n",
    "By default, Spark SQL uses the embedded deployment mode of a Hive metastore with a Apache Derby database.\n",
    "\n",
    "When SparkSession is created with Hive support the external catalog (aka metastore) is HiveExternalCatalog. HiveExternalCatalog uses spark.sql.warehouse.dir directory for the location of the databases\n",
    "\n",
    "The benefits of using an external Hive metastore:\n",
    "* Allow multiple Spark applications (sessions) to access it concurrently\n",
    "* Allow a single Spark application to use table statistics without running \"ANALYZE TABLE\" every execution\n",
    "\n",
    "<font color=blue>*Spark SQL uses the Hive-specific configuration properties that further fine-tune the Hive integration, e.g. spark.sql.hive.metastore.version or spark.sql.hive.metastore.jars.*</font>\n",
    "\n",
    "**spark.sql.warehouse.dir** is a static configuration property that sets Hiveâ€™s hive.metastore.warehouse.dir property, i.e. the location of the Hive local/embedded metastore database (using Derby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table in Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_tbl = \"CREATE TABLE IF NOT EXISTS SPARKHIVE_{}(age INT, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\"\n",
    "sqlContext.sql(create_tbl.format(user_id.upper()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data in Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overwrite_tbl = \"LOAD DATA LOCAL INPATH '/mnt/home/{}/sampleData.txt' OVERWRITE INTO TABLE SPARKHIVE_{}\"\n",
    "insert_to_tbl = \"LOAD DATA LOCAL INPATH '/mnt/home/{}/sampleData.txt' INTO TABLE SPARKHIVE_{}\"\n",
    "overwrite_tbl = \"LOAD DATA INPATH '/user/{}/sampleData.txt' OVERWRITE INTO TABLE SPARKHIVE_{}\"\n",
    "\n",
    "final_stmt = overwrite_tbl.format(user_id.lower(),user_id.upper())\n",
    "sqlContext.sql(final_stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run queries on Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30| Brian|\n",
      "| 35|  Alex|\n",
      "| 45| Shyam|\n",
      "| 70| Trump|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_sql = \"select * from SPARKHIVE_{}\".format(user_id.upper())\n",
    "sqlContext.sql(select_sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncate Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_sql = \"TRUNCATE TABLE SPARKHIVE_{}\".format(user_id.upper())\n",
    "sqlContext.sql(truncate_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Existing tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|20181121_sparkhiv...|      false|\n",
      "| default|           sparkhive|      false|\n",
      "| default|    sparkhive_431591|      false|\n",
      "| default|sparkhive_edureka...|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = sqlContext.sql(\"SHOW TABLES\")\n",
    "tables.where(\"tableName like '%sparkhive%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_sql = \"DROP TABLE IF EXISTS SPARKHIVE_{}\".format(user_id.upper())\n",
    "sqlContext.sql(drop_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
